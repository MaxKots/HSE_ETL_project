# Учебный проект: ETL с Apache Airflow, PostgreSQL и MongoDB

## Что это за проект?

Этот проект — мой учебный эксперимент с ETL-процессами. Я решил разобраться, как можно автоматизировать перенос данных из MongoDB в PostgreSQL с помощью Apache Airflow. Если коротко, то я беру сырые данные из MongoDB, обрабатываю их и загружаю в PostgreSQL, чтобы потом можно было легко анализировать. 

### Что я использовал?
- **Apache Airflow** — мой главный помощник в автоматизации. Он управляет всеми процессами: когда данные извлекать, как их обрабатывать и куда загружать.
- **MongoDB** — здесь хранятся исходные данные: сессии пользователей, поисковые запросы, история цен, обращения в поддержку и т.д.
- **PostgreSQL** — сюда попадают уже обработанные данные, готовые для аналитики.
- **Генератор данных** — специальный сервис, который заполняет MongoDB случайными данными, чтобы было с чем работать.
- **Docker** — чтобы все это легко развернуть и не париться с настройками.
- **Python** — на нем написаны все скрипты для ETL и генерации данных.

Цель проекта — научиться настраивать и автоматизировать процессы обработки данных, а также строить витрины данных для аналитики.

## Как все устроено?

Проект состоит из нескольких частей:

```plaintext
HSE_ETL_project/
├── .env                        # Настройки окружения
├── docker-compose.yml          # Конфигурация Docker
├── README.md                   # Документвация
├── Task.md                     # Задание
│
├── airflow/                    
│   ├── Dockerfile              # Docker для airflow
│   ├── requirements.txt        # Нужные зависимости для airflow
│   ├── cfg/
│   │   └── airflow.cfg         # конфиг airflow
│   │
│   ├── dags/                   # непосредственно airflow даги
│   │   ├── sql/                # SQL запросы
│   │
│   ├── logs/                   # логи airflow
│   └── plugins/                # плагины airflow  
│
├── generator/                  # генератор данных Mongo
│   ├── Dockerfile              # Docker для генератора
│   ├── generate_data.py        # скрипт для генерации данных
│   └── requirements.txt        # нужные зависимости генератора
│
└── db/                         # Базы данных
    ├── mongo/                  # MongoDB
    └── postgres/               # PostgreSQL
```

## Как запустить проект?

1. **Клонируем репозиторий:**
   \`\`\`bash
   git clone git@github.com:MaxKots/HSE_ETL_project.git
   cd HSE_ETL_project
   \`\`\`
2. **Запускаем контейнеры:**
   \`\`\`bash
   docker-compose up -d
   \`\`\`
3. **Открываем Airflow в браузере:**
   \`\`\`
   http://localhost:8080
   \`\`\`
4. **Запускаем DAG'и:**
   - Заходим в интерфейс Airflow.
   - Вручную запускаем нужные DAG'и.

## Генератор данных

Этот сервис создает тестовые данные в MongoDB. Он заполняет базу случайными значениями, чтобы имитировать реальный поток данных.

### Как запустить генератор?
Генератор автоматически запускается после старта MongoDB и заполняет базу тестовыми данными. Количество записей можно настроить в `.env`-файле.

## Как работают пайплайны?

### 1. Пайплайны репликации данных

В airflow найдем пайплайны для переноса данных из MongoDB в PostgreSQL. Каждый DAG отвечает за свою таблицу

### Как это работает?
1. **Извлечение (`Extract`)**  
   - Данные забираются из MongoDB с помощью через pymongo.

2. **Трансформация (`Transform`)**  
   - Убираем дубликаты.
   - Приводим данные к нужным форматам.
   - Разбиваем сложные структуры на простые таблицы.

3. **Загрузка (`Load`)**  
   - Данные записываются в PostgreSQL.

---

### 2. Пайплайны для витрин данных

#### 2.1 Витрина активности пользователей (`mart.activity`)
**Что это?**  
Эта витрина показывает, насколько активны пользователи: сколько у них сессий, сколько запросов они делают, как часто обращаются в поддержку и т.д.

**Какие данные есть?**
- `user_id` — ID пользователя.
- `total_sessions` — общее количество сессий.
- `first_session_time` — время первой сессии.
- `last_session_time` — время последней сессии.
- `avg_session_duration_min` — средняя длительность сессии.
- `total_search_queries` — количество поисковых запросов.
- `support_tickets_count` — количество обращений в поддержку.
- `recommended_products_count` — количество рекомендаций.

#### 2.2 Витрина эффективности поддержки (`mart.efficiency`)
**Что это?**  
Эта витрина показывает, как быстро и эффективно работает поддержка: сколько тикетов обрабатывается, сколько времени уходит на решение проблем и т.д.

**Какие данные есть?**
- `created_date` — дата создания тикета.
- `issue_type` — тип проблемы.
- `total_tickets` — общее количество тикетов.
- `open_tickets` — количество открытых тикетов.
- `closed_tickets` — количество закрытых тикетов.
- `avg_resolution_time_minutes` — среднее время решения.

### 3. Автопайплайны

#### 3.1 Автопайплайн для витрины активности пользователей.
**DAG:** `run_activity_pipeline`  

**Как работает?**
1. Запускает DAG'и репликации данных.
2. Ждет их завершения.
3. Запускает DAG для обновления витрины.

#### 3.2 Автопайплайн для витрины эффективности поддержки.
**DAG:** `run_efficiency_pipeline`  

**Как работает?**
1. Запускает DAG репликации данных.
2. Ждет его завершения.
3. Запускает DAG для обновления витрины.
